---
title: "Predicting barbell lifts from ‘quantified self’ devices"
author: "J.S. Ramos"
date: "September 26, 2015"
output: html_document
---

# 1. Introduction

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity. These type of devices are part of the quantified self movement. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, our goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants collected while perform barbell lifts correctly and incorrectly in order to predict whether or not these lifts were done correctly. This is the "classe" variable in the data sets.

We would like to thank the 'Human Activity Recognition' group of the Informatics Department at the Pontifical Catholic University of Rio de Janeiro for making this dataset available for this study.

# 2. Data loading, reduction and exploration

```{r, echo=FALSE, results='hide', warning=FALSE, message=FALSE}
options(scipen=999)
set.seed(123123)
library(dplyr)
library(lattice)
library(ggplot2)
library(randomForest)
library(caret)
library(doMC)
```

Though the assignment explicitly provides both training and test sets, for the purpose of this study we will partition just the training to generate the test set, and the original test set will be used as validation set.

The training/test set can be found [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv), and the validation can be downloaded from [here](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv). We will download both locally to avoid network overhead, and explore and process the training set first. Note that it will be necessary to transform the `classe` var into a factor, and that doing this in the original training set will cause the generated test set to have it as factor as well.

```{r}
dat <- read.csv('./pml-training.csv', 
                header = T, 
                sep=",",
                na.strings = c("#DIV/0!","NA",""), 
                strip.white = T, 
                stringsAsFactors=F)
validation <- read.csv('./pml-testing.csv', 
                header = T, 
                sep=",",
                na.strings = c("#DIV/0!","NA",""), 
                strip.white = T, 
                stringsAsFactors=F)
dat$classe <- as.factor(dat$classe)
```

As explained throughout the course, the first and foremost step in building a statistical learning model is to partition the data. We will have `70%` of the original training set as training, and `30%` of it as test set. *Note that the validation set will not be touched beyond this point*.

```{r}
inTrain <- createDataPartition(y=dat$classe, p=0.7, list=F)
training <- dat[inTrain,] # Don't forget the commas!
test <- dat[-inTrain,] # Don't forget the commas!
```

We end up with ``r nrow(training)`` rows for the training, ``r nrow(test)`` for the test set, and ``r nrow(validation)`` for the validation test. Upon first inspection, we find that the following columns have mostly `NA` observations, which means we should only consider those columns that have enough information. We have determined that relevant variables are those with at least `30%` of non-NAs. *This and subsequent transformations will only happen on the training set, since both test and validation sets must remain untouched.*

```{r}
redTraining <- training[,colMeans(!is.na(training)) >= 0.3]
```

We also remove the `X` since, being just a row id, it will have ``r (length(unique(redTraining$X))/length(redTraining$X))*100`` variance and this will have an effect on our prediction. We'll also remove the `user_name` column since, being the subject's name, it doesn't add information to the model. Finally, we remove the `*timestamp*` and the `*window*` columns, since they are time-related variables and we do not want a prediction model on a time-series.

```{r}
finalTraining <- redTraining[,-c(1:7)]
```

With this we have gone from ``r ncol(dat)`` to ``r ncol(finalTraining)``. We can now proceed to feature and model selection.

# 3. Model building

We choose a *Random Forest* model due to it having built-in feature selection. According to the `caret` package [documentation](http://topepo.github.io/caret/featureselection.html), its feature selection algorithm is coupled with the parameter estimation algorithm, making it faster than if the features were searched for externally.

Also, even though [some authors](https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm#ooberr) state that RFs do not require cross-validation, we will nonetheless train our RF with a 5-fold, single-pass cross-validation in order to reduce any potential bias, and to further get a more accurate estimate of the *out-of-sample error*.

```{r, warning=FALSE, message=FALSE}
registerDoMC(cores = 3) # Register core for parallel processing.
tControl <- trainControl(method='cv', number = 5, allowParallel = T) # train control function for X-validation.
ptm <- proc.time()
rfModel <- train(classe ~ ., data = finalTraining, method='rf', trControl=tControl, importance = T) # model building
finalTime <- proc.time() - ptm
rfModel
```

We can also prove that the Random Forest algorithm is computationally expensive by looking at the following table, where it shows that on an Intel Quad-core i7 with 16GB RAM and SSD the model trained in `12 mins`, even though we registered 3 cores for parallel execution. Doing it with a single core increases that time to `37 mins`.

```{r, warning=FALSE, message=FALSE}
finalTime
```

## 3.1 Selected variables

```{r,echo=F, warning=FALSE, message=FALSE, fig.align='center', fig.height=6, fig.width=7}
varImpPlot(rfModel$finalModel, sort = T, n.var = 20, type = 2, 
           main='Top 20 most important variables', pch=19, lcolor='black')
```

According to the plot, RF's built-in feature selection has determined that the variables that most contribute to decrease model impurity are `roll_belt`, `yaw_belt`, `magnet_dumbbell_z` and `magnet_dumbbell_y`. This means that it would be possible to build a random forest with these variables at the expense of small decrease in accuracy and a small increase in OOB error.

## 3.2 Accuracy and in-sample error

```{r}
trainingConf <- rfModel$finalModel$confusion
trainingConf
```

From the fitted model above, we can see that we've achieved an **in-sample accuracy** measure of ``r round(mean(1-trainingConf[,6])*100,2)``% for mtry = ``r rfModel$finalModel$mtry`` (vars sampled as potential splits). with an **in-sample error** of ``r round(mean(trainingConf[,6]),3)*100``%.

# 4. Model performance

We will now apply the model to the test set and explore its performance by looking at the **out-of-sample error** and the **accuracy**. Remember that our process of stripping variables was only applied to the training set.

## 4.1 Prediction accuracy and error on test set

```{r}
pred <- predict(rfModel, test)
testConf <- confusionMatrix(pred, test$classe)
testConf
```

As we can see, we've achieved an **accuracy** of ``r round(testConf$overall[1]*100, 2)``% and an **out-of-sample error** of ``r round((1-testConf$overall[1])*100, 2)``%. As we can observe, these figures fall closely in line with the **in-sample** error and accuracy. The general expectation is for the **out-of-sample** metrics to be much greater. This difference is curious, but not entirely unexpected, due to any of the following:

1. The training set having more diversified cases and hence accounts for more variance. 
2. The cross-validation, as expected, is reducing the bias, but marginally increasing the variance. 
3. it is possible that there is no significant difference (in the statistical sense) between our prediction's ``r round((1-testConf$overall[1])*100, 2)``% and the model's ``r round(mean(trainingConf[,6]),3)*100``% sample error.

## 4.2 Prediction on an entirely new dataset

We have called this test set the 'validation set', and even though we don't have the correct predictions in it, we will nonetheless apply our model to it.

```{r}
blindPred <- predict(rfModel, validation)
blindPred
```

We will convert the predictions to `chr` data type and will generate 20 text files with the individual prediction for each case for submission purposes.

```{r}
finalBlindPred <- as.character(blindPred)

pml_write_files = function(x) {
        n = length(x)
        for(i in 1:n){
                filename = paste0("problem_id_",i,".txt")
                write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)
        }
}

pml_write_files(finalBlindPred)
```

This concludes our study